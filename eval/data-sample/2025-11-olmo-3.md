# Olmo 3: Charting a path through the model flow to lead open-source AI

**Date:** November 2025  
**Source:** [Allen Institute for AI](https://allenai.org/blog/olmo3)

## Summary

Olmo 3 is a family of fully open language models designed to provide a complete and transparent pipeline from data to deployment. It empowers the open source community with state-of-the-art open models, the entire model flow, and full traceability back to training data.

## Model Variants

- **Olmo 3 32B-Base**: Excels in programming, reading comprehension, and math problem-solving; maintains performance at extended context lengths
- **Olmo 3 32B-Think**: Capable of step-by-step reasoning through complex problems; robust platform for reinforcement learning research
- **Olmo 3 32B-Instruct**: Instruction-tuned model built for chat, tool use, and multi-turn dialogue
- **Olmo 3 7B-Base**: Smaller model suitable for wider range of hardware while delivering competitive performance
- **Olmo 3 7B-Think**: Strong reasoning capabilities at 7B scale, efficiently handles complex prompts

## Efficient Training Stack

- Pretrained on cluster of up to 1,024 H100 GPUs
- Training throughput: 7.7K tokens per device per second for Olmo 3-Base (7B)
- 8x throughput increase by integrating SFT into Olmo Core
- 4x more efficient RL training through in-flight weight updates and continuous batching

## Transparency and Open-Source Tools

### OlmoTrace Integration
Traces model outputs back to training data in real time, enhancing transparency and explainability.

### Open-Source Tools Provided
- **OlmoCore**: Distributed model training
- **Open Instruct**: Post-training
- **datamap-rs**: Large-scale cleaning
- **duplodocus**: Fuzzy de-duplication
- **OLMES**: Reproducible evaluations
- **decon**: Removing test sets from training data

## Data Availability

All training and fine-tuning datasets available for download without license restrictions, allowing for custom deployment and reuse.
