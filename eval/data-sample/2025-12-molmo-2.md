# Molmo 2: State-of-the-art video understanding, pointing, and tracking

**Date:** December 11, 2025  
**Source:** [Allen Institute for AI](https://allenai.org/blog/molmo2)

## Summary

Molmo 2 is a new suite of state-of-the-art vision-language models with open weights, training data, and training code that can analyze videos and multiple images at once. It extends Molmo's strengths in grounded vision to video and multi-image understanding.

## Model Variants

- **Molmo 2 (8B)**: Qwen 3-based, best overall model for video grounding and QA
- **Molmo 2 (4B)**: Qwen 3-based, optimized for efficiency
- **Molmo 2-O (7B)**: Built on Olmo, fully open end-to-end model flow including the underlying LLM

## Key Capabilities

### State-of-the-art Performance
- Leads or ties for best results among open peers on image QA, short-video QA, video counting, video tracking, and human preference
- Molmo 2 (8B) outperforms the original Molmo (72B) on key image pointing and grounding benchmarks
- On video tracking, leapfrogs Gemini 3 Pro and strong open-weight alternatives

### Video Grounding Features
- **Counting-by-pointing**: Returns points and timestamps for events
- **Multi-object tracking**: Persistent IDs that follow objects across occlusions and re-entries
- **Dense video captioning**: Highly descriptive and searchable narratives of long clips
- **Anomaly detection**: Flags rare or surprising events
- **Artifact detection**: Points to flaws in generative video
- **Subtitle-aware QA**: Combines visual evidence with in-video subtitles

## Architecture

The architecture consists of:
- Vision encoder that processes images or video frames into visual tokens
- Language model backbone (Qwen 3 or Olmo)
- Lightweight connector that interleaves visual tokens with timestamps, image indices, and text

### Training
- Two-stage training process: pretraining for alignment/grounding, then SFT on multimodal mixture
- Samples up to 128 frames at â‰¤2 fps per clip
- Trained on less than one-eighth of the video data used by Meta's PerceptionLM (9.19M vs 72.5M videos)

## New Datasets

| Dataset | Description | Scale |
|---------|-------------|-------|
| Molmo2-Cap | Dense video captioning | 104k videos + 431k clips |
| Molmo2-AskModelAnything | Human-authored video QA pairs | 140k QA pairs |
| Molmo2-CapQA | Synthetic video QA from captions | 1M QA pairs (200k videos) |
| Molmo2-SubtitleQA | Video QA with transcripts | 300k QA pairs (100k videos) |
| Molmo2-VideoPoint | Video pointing for localization | 300k+ queries (160k videos) |
| Molmo2-VideoTrack | Point-based object tracking | 3.6k clips; 15k queries |
| Molmo2-MultiImageQA | QA over related image sets | 45k sets; 72k QA |
| Molmo2-MultiImagePoint | Multi-image pointing + counting | 470k+ samples |
| Molmo2-SynMultiImageQA | Synthetic multi-image QA | 188k examples |

## License

Licensed under Apache 2.0. Trained on third-party datasets subject to academic and non-commercial research use only.
