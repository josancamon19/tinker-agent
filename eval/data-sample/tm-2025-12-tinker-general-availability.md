# Tinker: General Availability and Vision Input

**Date:** December 12, 2025  
**Source:** [Thinking Machines Lab](https://thinkingmachines.ai/blog/tinker-general-availability/)

## Summary

Thinking Machines Lab announced several significant updates to Tinker, transitioning from a waitlist to being openly accessible to all users.

## Key Updates

### 1. General Availability
Tinker is now openly accessible to all users without a waitlist. Anyone can sign up and start using the platform immediately.

### 2. Kimi K2 Thinking Model
The Kimi K2 Thinking model was added to Tinker's lineup:
- One trillion parameters
- Designed for complex reasoning tasks
- Optimized for tool utilization

### 3. OpenAI API Compatibility
Tinker introduced an inference interface compatible with the OpenAI API, facilitating seamless integration with platforms that support this API.

### 4. Vision Input Support
Tinker expanded capabilities to include vision inputs with two vision-language models:
- **Qwen3-VL-30B-A3B-Instruct**
- **Qwen3-VL-235B-A22B-Instruct**

These models enable users to process:
- Images
- Screenshots
- Diagrams

## Vision Capabilities Demo

Thinking Machines Lab fine-tuned the Qwen3-VL-235B-A22B-Instruct model to classify images across four classic datasets:
- Caltech 101
- Stanford Cars
- Oxford Flowers
- Oxford Pets

### Results
In scenarios with limited data, Qwen3-VL-235B-A22B-Instruct outperformed traditional vision-only models like DINOv2. This advantage is attributed to its inherent language understanding, which enhances performance in vision tasks beyond simple classification.
